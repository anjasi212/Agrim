{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8cf57-f30d-4417-92d6-7e240c08005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet neo4j langchain-community langchain-core langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1a8a0-20a4-47b6-a3c0-79faf9917fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import add\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import getpass\n",
    "from typing import List, Dict, Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39290339-ed6c-40ab-a856-f429ea0eda32",
   "metadata": {},
   "source": [
    "# Implementing GraphReader with Neo4j and LangGraph\n",
    "\n",
    "Large Language Models (LLMs) are great at traditional NLP tasks like summarization and sentiment analysis but the stronger models also demonstrate promising reasoning abilities. LLM reasoning is often understood as the ability to tackle complex problems by formulating a plan, executing it, and assessing progress at each step. Based on this evaluation, they can adapt by revising the plan or taking alternative actions. The rise of agents is becoming an increasingly compelling approach to answering complex questions in RAG applications.\n",
    "\n",
    "In this blog post, we’ll explore the implementation of the GraphReader agent. This agent is designed to retrieve information from a structured knowledge graph that follows a predefined schema. Unlike the typical graphs you might see in presentations, this one is closer to a document or lexical graph, containing documents, their chunks, and relevant metadata in the form of atomic facts.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aws90bkQ8PPNYFFIbXdMpA.png)\n",
    "\n",
    "The image above illustrates a knowledge graph, beginning at the top with a document node labeled Joan of Arc. This document is broken down into text chunks, represented by numbered circular nodes (0, 1, 2, 3), which are connected sequentially through NEXT relationships, indicating the order in which the chunks appear in the document. Below the text chunks, the graph further breaks down into atomic facts, where specific statements about the content are represented. Finally, at the bottom level of the graph, we see the key elements, represented as circular nodes with topics like historical icons, Dane, French nation, and France. These elements act as metadata, linking the facts to the broader themes and concepts relevant to the document.\n",
    "\n",
    "Once we have constructed the knowledge graph, we will follow the implementation provided in the GraphReader paper.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PokbiJwPnZ6Fndfo9tPwbg.png)\n",
    "\n",
    "The agent exploration process involves initializing the agent with a rational plan and selecting initial nodes to start the search in a graph. The agent explores these nodes by first gathering atomic facts, then reading relevant text chunks, and updating its notebook. The agent can decide to explore more chunks, neighboring nodes, or terminate based on gathered information. When the agent decided to terminate, the answer reasoning step is executed to generate the final answer.\n",
    "\n",
    "In this blog post, we will implement the GraphReader paper using Neo4j as the storage layer and LangChain in combination with LangGraph to define the agent and its flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb835be5-c1d1-422f-8ff8-7be3f5205395",
   "metadata": {},
   "source": [
    "**Make sure to run the graphreader import notebook first!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815dc99-6b4c-4033-8f31-9b98678b3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEO4J_URI\"] = \"neo4j+s://089bfb9e.databases.neo4j.io\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"jsSYcLP-a_JvuHYh7xKf5890FOrn1El5BvlXUQGr8hM\"\n",
    "\n",
    "neo4j_graph = Neo4jGraph(refresh_schema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9f604-4e51-471a-b94d-b0e9c12427fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "#\"gpt-4-0125-preview\"\n",
    "model = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.2)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd65269-2823-4593-92ed-66a32730bfa9",
   "metadata": {},
   "source": [
    "# GraphReader Agent\n",
    "We’re ready to implement GraphReader, a graph-based agent system. The agent starts with a couple of predefined steps, followed by the steps in which it can traverse the graph autonomously, meaning the agent decides the following steps and how to traverse the graph.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:740/format:webp/1*brUZD1mkh9tDg8AwMEmxTw.png)\n",
    "\n",
    "The process begins with a rational planning stage, after which the agent makes an initial selection of nodes (key elements) to work with. Next, the agent checks atomic facts linked to the selected key elements. Since all these steps are predefined, they are visualized with a full line.\n",
    "\n",
    "Depending on the outcome of the atomic fact check, the flow proceeds to either read relevant text chunks or explore the neighbors of the initial key elements in search of more relevant information. Here, the next step is conditional and based on the results of an LLM and is, therefore, visualized with a dotted line.\n",
    "\n",
    "In the chunk check stage, the LLM reads and evaluates whether the information gathered from the current text chunk is sufficient. Based on this evaluation, the LLM has a few options. It can decide to read additional text chunks if the information seems incomplete or unclear. Alternatively, the LLM may choose to explore neighboring key elements, looking for more context or related information that the initial selection might not have captured. If, however, the LLM determines that enough relevant information has been gathered, it will proceed directly to the answer reasoning step. At this point, the LLM generates the final answer based on the collected information.\n",
    "\n",
    "Throughout this process, the agent dynamically navigates the flow based on the outcomes of the conditional checks, making decisions on whether to repeat steps or continue forward depending on the specific situation. This provides flexibility in handling different inputs while maintaining a structured progression through the steps.\n",
    "\n",
    "Now, we’ll go over the steps and implement them using LangGraph abstraction. You can learn more about LangGraph through LangChain’s academy course.\n",
    "\n",
    "## LangGraph state\n",
    "To build a LangGraph implementation, we start by defining a state passed along the steps in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a393cb-945d-4047-b57e-5d66e6ac0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "    analysis: str\n",
    "    previous_actions: List[str]\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    question: str\n",
    "    rational_plan: str\n",
    "    notebook: str\n",
    "    previous_actions: Annotated[List[str], add]\n",
    "    check_atomic_facts_queue: List[str]\n",
    "    check_chunks_queue: List[str]\n",
    "    neighbor_check_queue: List[str]\n",
    "    chosen_action: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0482101-455a-4247-8295-aeaaec376a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(input_str):\n",
    "    # Regular expression to capture the function name and arguments\n",
    "    pattern = r'(\\w+)(?:\\((.*)\\))?'\n",
    "    \n",
    "    match = re.match(pattern, input_str)\n",
    "    if match:\n",
    "        function_name = match.group(1)  # Extract the function name\n",
    "        raw_arguments = match.group(2)  # Extract the arguments as a string        \n",
    "        # If there are arguments, attempt to parse them\n",
    "        arguments = []\n",
    "        if raw_arguments:\n",
    "            try:\n",
    "                # Use ast.literal_eval to safely evaluate and convert the arguments\n",
    "                parsed_args = ast.literal_eval(f'({raw_arguments})')  # Wrap in tuple parentheses\n",
    "                # Ensure it's always treated as a tuple even with a single argument\n",
    "                arguments = list(parsed_args) if isinstance(parsed_args, tuple) else [parsed_args]\n",
    "            except (ValueError, SyntaxError):\n",
    "                # In case of failure to parse, return the raw argument string\n",
    "                arguments = [raw_arguments.strip()]\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'function_name': function_name,\n",
    "            'arguments': arguments\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5843878-dd36-4901-b085-fb5d8be69bdd",
   "metadata": {},
   "source": [
    "For more advanced use cases, multiple separate states can be used. In our implementation, we have separate input and output states, which define the input and output of the LangGraph, and a separate overall state, which is passed between steps.\n",
    "\n",
    "By default, the state is overwritten when returned from a node. However, you can define other operations. For example, with the previous_actions we define that the state is appended or added instead of overwritten.\n",
    "\n",
    "The agent begins by maintaining a notebook to record supporting facts, which are eventually used to derive the final answer. Other states will be explained as we go along.\n",
    "\n",
    "Let’s move on to defining the nodes in the LangGraph.\n",
    "\n",
    "## Rational plan\n",
    "In the rational plan step, the agent breaks the question into smaller steps, identifies the key information required, and creates a logical plan. The logical plan allows the agent to handle complex multi-step questions.\n",
    "\n",
    "While the code is unavailable, all the prompts are in the appendix, so we can easily copy them.\n",
    "The authors don’t explicitly state whether the prompt is provided in the system or user message. For the most part, I have decided to put the instructions as a system message.\n",
    "\n",
    "The following code shows how to construct a chain using the above rational plan as the system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce402f5-3a3b-44e7-be41-a9ed8e8010ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rational_plan_system = \"\"\"As an intelligent assistant, your primary objective is to answer the question by gathering\n",
    "supporting facts from a given article. To facilitate this objective, the first step is to make\n",
    "a rational plan based on the question. This plan should outline the step-by-step process to\n",
    "resolve the question and specify the key information required to formulate a comprehensive answer.\n",
    "Example:\n",
    "#####\n",
    "User: Who had a longer tennis career, Danny or Alice?\n",
    "Assistant: In order to answer this question, we first need to find the length of Danny’s\n",
    "and Alice’s tennis careers, such as the start and retirement of their careers, and then compare the\n",
    "two.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin.\"\"\"\n",
    "\n",
    "rational_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            rational_plan_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"{question}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rational_chain = rational_prompt | model | StrOutputParser()\n",
    "\n",
    "def rational_plan_node(state: InputState) -> OverallState:\n",
    "    rational_plan = rational_chain.invoke({\"question\": state.get(\"question\")})\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Step: rational_plan\")\n",
    "    print(f\"Rational plan: {rational_plan}\")\n",
    "    return {\n",
    "        \"rational_plan\": rational_plan,\n",
    "        \"previous_actions\": [\"rational_plan\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb952ad-81c0-4103-9db9-f7ce59f3090c",
   "metadata": {},
   "source": [
    "The function starts by invoking the LLM chain, which produces the rational plan. We do a little printing for debugging and then update the state as the function’s output. I like the simplicity of this approach.\n",
    "\n",
    "## Initial node selection\n",
    "In the next step, we select the initial nodes based on the question and rational plan. The prompt starts by giving the LLM some context about the overall agent system, followed by the task instructions. The idea is to have the LLM select the top 10 most relevant nodes and score them. The authors simply put all the key elements from the database in the prompt for an LLM to select from. However, I think that approach doesn’t really scale. Therefore, we will create and use a vector index to retrieve a list of input nodes for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdee6c-acaa-4e94-9998-4ec311ca38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_vector = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    index_name=\"keyelements\",\n",
    "    node_label=\"KeyElement\",\n",
    "    text_node_properties=[\"id\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    "    retrieval_query=\"RETURN node.id AS text, score, {} AS metadata\"\n",
    ")\n",
    "\n",
    "def get_potential_nodes(question: str) -> List[str]:\n",
    "    data = neo4j_vector.similarity_search(question, k=50)\n",
    "    return [el.page_content for el in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b26445-09e0-4c5f-91ed-97aa8aacae38",
   "metadata": {},
   "source": [
    "The from_existing_graph method pulls the defined text_node_properties from the graph and calculates embeddings where they are missing. Here, we simply embed the id property of KeyElement nodes.\n",
    "\n",
    "Now let’s define the chain. We’ll first copy the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e34514-616a-416b-8907-c4b4abe2e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_node_system = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
    "comprising the following elements:\n",
    "1. Text Chunks: Chunks of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the\n",
    "rational plan, and a list of node key elements. These initial nodes are crucial because they are the\n",
    "starting point for searching for relevant information.\n",
    "Requirements:\n",
    "#####\n",
    "1. Once you have selected a starting node, assess its relevance to the potential answer by assigning\n",
    "a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,\n",
    "whereas a score of 0 suggests minimal relevance.\n",
    "2. Present each chosen starting node in a separate line, accompanied by its relevance score. Format\n",
    "each line as follows: Node: [Key Element of Node], Score: [Relevance Score].\n",
    "3. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.\n",
    "4. In the user’s input, each line constitutes a node. When selecting the starting node, please make\n",
    "your choice from those provided, and refrain from fabricating your own. The nodes you output\n",
    "must correspond exactly to the nodes given by the user, with identical wording.\n",
    "Finally, I emphasize again that you need to select the starting node from the given Nodes, and\n",
    "it must be consistent with the words of the node you selected. Please strictly follow the above\n",
    "format. Let’s begin.\n",
    "\"\"\"\n",
    "\n",
    "initial_node_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            initial_node_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Nodes: {nodes}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db91e3-4f32-43ab-91f7-8f84b77e5b96",
   "metadata": {},
   "source": [
    "Again, we put most of the instructions as the system message. Since we have multiple inputs, we can define them in the human message. However, we need a more structured output this time. Instead of writing a parsing function that takes in text and outputs a JSON, we can simply use the use_structured_outputmethod to define the desired output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375fb1c-087f-4598-85a7-6ef080aa3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(BaseModel):\n",
    "    key_element: str = Field(description=\"\"\"Key element or name of a relevant node\"\"\")\n",
    "    score: int = Field(description=\"\"\"Relevance to the potential answer by assigning\n",
    "a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,\n",
    "whereas a score of 0 suggests minimal relevance.\"\"\")\n",
    "\n",
    "class InitialNodes(BaseModel):\n",
    "    initial_nodes: List[Node] = Field(description=\"List of relevant nodes to the question and plan\")\n",
    "\n",
    "initial_nodes_chain = initial_node_prompt | model.with_structured_output(InitialNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947cb81-bbf9-4ce5-9fc5-78b99cd6ff52",
   "metadata": {},
   "source": [
    "We want to output a list of nodes containing the key element and the score. We can easily define the output using a Pydantic model. Additionally, it is vital to add descriptions to each of the field, so we can guide the LLM as much as possible.\n",
    "\n",
    "The last thing in this step is to define the node as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c28e9-b5cb-4fdc-ba6e-401b5642e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_node_selection(state: OverallState) -> OverallState:\n",
    "    potential_nodes = get_potential_nodes(state.get(\"question\"))\n",
    "    initial_nodes = initial_nodes_chain.invoke(\n",
    "        {\n",
    "            \"question\": state.get(\"question\"),\n",
    "            \"rational_plan\": state.get(\"rational_plan\"),\n",
    "            \"nodes\": potential_nodes,\n",
    "        }\n",
    "    )\n",
    "    # paper uses 5 initial nodes\n",
    "    check_atomic_facts_queue = [\n",
    "        el.key_element\n",
    "        for el in sorted(\n",
    "            initial_nodes.initial_nodes,\n",
    "            key=lambda node: node.score,\n",
    "            reverse=True,\n",
    "        )\n",
    "    ][:5]\n",
    "    return {\n",
    "        \"check_atomic_facts_queue\": check_atomic_facts_queue,\n",
    "        \"previous_actions\": [\"initial_node_selection\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf07c1-efd7-42ca-b2f2-b470c3e39659",
   "metadata": {},
   "source": [
    "In the initial node selection, we start by getting a list of potential nodes using the vector similarity search based on the input. An option is to use rational plan instead. The LLM is prompted to output the 10 most relevant nodes. However, the authors say that we should use only 5 initial nodes. Therefore, we simply order the nodes by their score and take the top 5 ones. We then update the check_atomic_facts_queue with the selected initial key elements.\n",
    "\n",
    "## Atomic fact check\n",
    "In this step, we take the initial key elements and inspect the linked atomic facts. All prompts start by giving the LLM some context, followed by task instructions. The LLM is instructed to read the atomic facts and decide whether to read the linked text chunks or if the atomic facts are irrelevant, search for more information by exploring the neighbors. The last bit of the prompt is the output instructions. We will use the structured output method again to avoid manually parsing and structuring the output.\n",
    "\n",
    "Since chains are very similar in their implementation, different only by prompts, we’ll avoid showing every definition in this blog post. However, we’ll look at the LangGraph node definitions to better understand the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c98b-4a26-4cc2-bb4b-a5802f73e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_fact_check_system = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
    "comprising the following elements:\n",
    "1. Text Chunks: Chunks of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to check a node and its associated atomic facts, with the objective of\n",
    "determining whether to proceed with reviewing the text chunk corresponding to these atomic facts.\n",
    "Given the question, the rational plan, previous actions, notebook content, and the current node’s\n",
    "atomic facts and their corresponding chunk IDs, you have the following Action Options:\n",
    "#####\n",
    "1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
    "fact may hold the necessary information to answer the question. This will allow you to access\n",
    "more complete and detailed information.\n",
    "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
    "information.\n",
    "#####\n",
    "Strategy:\n",
    "#####\n",
    "1. Reflect on previous actions and prevent redundant revisiting nodes or chunks.\n",
    "2. You can choose to read multiple text chunks at the same time.\n",
    "3. Atomic facts only cover part of the information in the text chunk, so even if you feel that the\n",
    "atomic facts are slightly relevant to the question, please try to read the text chunk to get more\n",
    "complete information.\n",
    "#####\n",
    "Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the\n",
    "question, you should still look at the text chunk to avoid missing information. You should only\n",
    "choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to\n",
    "the question. Please strictly follow the above format. Let’s begin.\n",
    "\"\"\"\n",
    "\n",
    "class AtomicFactOutput(BaseModel):\n",
    "    updated_notebook: str = Field(description=\"\"\"First, combine your current notebook with new insights and findings about\n",
    "the question from current atomic facts, creating a more complete version of the notebook that\n",
    "contains more valid information.\"\"\")\n",
    "    rational_next_action: str = Field(description=\"\"\"Based on the given question, the rational plan, previous actions, and\n",
    "notebook content, analyze how to choose the next action.\"\"\")\n",
    "    chosen_action: str = Field(description=\"\"\"1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
    "fact may hold the necessary information to answer the question. This will allow you to access\n",
    "more complete and detailed information.\n",
    "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
    "information.\"\"\")\n",
    "\n",
    "atomic_fact_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            atomic_fact_check_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Previous actions: {previous_actions}\n",
    "Notebook: {notebook}\n",
    "Atomic facts: {atomic_facts}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "atomic_fact_chain = atomic_fact_check_prompt | model.with_structured_output(AtomicFactOutput)\n",
    "\n",
    "def get_atomic_facts(key_elements: List[str]) -> List[Dict[str, str]]:\n",
    "    data = neo4j_graph.query(\"\"\"\n",
    "    MATCH (k:KeyElement)<-[:HAS_KEY_ELEMENT]-(fact)<-[:HAS_ATOMIC_FACT]-(chunk)\n",
    "    WHERE k.id IN $key_elements\n",
    "    RETURN distinct chunk.id AS chunk_id, fact.text AS text\n",
    "    \"\"\", params={\"key_elements\": key_elements})\n",
    "    return data\n",
    "\n",
    "def get_neighbors_by_key_element(key_elements):\n",
    "    print(f\"Key elements: {key_elements}\")\n",
    "    data = neo4j_graph.query(\"\"\"\n",
    "    MATCH (k:KeyElement)<-[:HAS_KEY_ELEMENT]-()-[:HAS_KEY_ELEMENT]->(neighbor)\n",
    "    WHERE k.id IN $key_elements AND NOT neighbor.id IN $key_elements\n",
    "    WITH neighbor, count(*) AS count\n",
    "    ORDER BY count DESC LIMIT 50\n",
    "    RETURN collect(neighbor.id) AS possible_candidates\n",
    "    \"\"\", params={\"key_elements\":key_elements})\n",
    "    return data\n",
    "\n",
    "def atomic_fact_check(state: OverallState) -> OverallState:\n",
    "    atomic_facts = get_atomic_facts(state.get(\"check_atomic_facts_queue\"))\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Step: atomic_fact_check\")\n",
    "    print(\n",
    "        f\"Reading atomic facts about: {state.get('check_atomic_facts_queue')}\"\n",
    "    )\n",
    "    atomic_facts_results = atomic_fact_chain.invoke(\n",
    "        {\n",
    "            \"question\": state.get(\"question\"),\n",
    "            \"rational_plan\": state.get(\"rational_plan\"),\n",
    "            \"notebook\": state.get(\"notebook\"),\n",
    "            \"previous_actions\": state.get(\"previous_actions\"),\n",
    "            \"atomic_facts\": atomic_facts,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    notebook = atomic_facts_results.updated_notebook\n",
    "    print(\n",
    "        f\"Rational for next action after atomic check: {atomic_facts_results.rational_next_action}\"\n",
    "    )\n",
    "    chosen_action = parse_function(atomic_facts_results.chosen_action)\n",
    "    print(f\"Chosen action: {chosen_action}\")\n",
    "    response = {\n",
    "        \"notebook\": notebook,\n",
    "        \"chosen_action\": chosen_action.get(\"function_name\"),\n",
    "        \"check_atomic_facts_queue\": [],\n",
    "        \"previous_actions\": [\n",
    "            f\"atomic_fact_check({state.get('check_atomic_facts_queue')})\"\n",
    "        ],\n",
    "    }\n",
    "    if chosen_action.get(\"function_name\") == \"stop_and_read_neighbor\":\n",
    "        neighbors = get_neighbors_by_key_element(\n",
    "            state.get(\"check_atomic_facts_queue\")\n",
    "        )\n",
    "        response[\"neighbor_check_queue\"] = neighbors\n",
    "    elif chosen_action.get(\"function_name\") == \"read_chunk\":\n",
    "        response[\"check_chunks_queue\"] = chosen_action.get(\"arguments\")[0]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3812c-5c2c-4955-a4ea-1da7fc2868e5",
   "metadata": {},
   "source": [
    "The atomic fact check node starts by invoking the LLM to evaluate the atomic facts of the selected nodes. Since we are using the use_structured_output we can parse the updated notebook and the chosen action output in a straightforward manner. If the selected action is to get additional information by inspecting the neighbors, we use a function to find those neighbors and append them to the check_atomic_facts_queue. Otherwise, we append the selected chunks to the check_chunks_queue. We update the overall state by updating the notebook, queues, and the chosen action.\n",
    "\n",
    "## Text chunk check\n",
    "As you might imagine by the name of the LangGraph node, in this step, the LLM reads the selected text chunk and decides the best next step based on the provided information. The LLM is instructed to read the text chunk and decide on the best approach. My gut feeling is that sometimes relevant information is at the start or the end of a text chunk, and parts of the information might be missing due to the chunking process. Therefore, the authors decided to give the LLM the option to read a previous or next chunk. If the LLM decides it has enough information, it can hop on to the final step. Otherwise, it has the option to search for more details using the search_more function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02237ea5-0567-4e9d-8e76-de942e942f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_read_system_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
    "following elements:\n",
    "1. Text Chunks: Segments of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to assess a specific text chunk and determine whether the available information\n",
    "suffices to answer the question. Given the question, rational plan, previous actions, notebook\n",
    "content, and the current text chunk, you have the following Action Options:\n",
    "#####\n",
    "1. search_more(): Choose this action if you think that the essential information necessary to\n",
    "answer the question is still lacking.\n",
    "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
    "valuable information for answering the question.\n",
    "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
    "valuable information for answering the question.\n",
    "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
    "is enough to answer the question. This will allow you to summarize the gathered information and\n",
    "provide a final answer.\n",
    "#####\n",
    "Strategy:\n",
    "#####\n",
    "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
    "2. You can only choose one action.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin\n",
    "\"\"\"\n",
    "\n",
    "class ChunkOutput(BaseModel):\n",
    "    updated_notebook: str = Field(description=\"\"\"First, combine your previous notes with new insights and findings about the\n",
    "question from current text chunks, creating a more complete version of the notebook that contains\n",
    "more valid information.\"\"\")\n",
    "    rational_next_move: str = Field(description=\"\"\"Based on the given question, rational plan, previous actions, and\n",
    "notebook content, analyze how to choose the next action.\"\"\")\n",
    "    chosen_action: str = Field(description=\"\"\"1. search_more(): Choose this action if you think that the essential information necessary to\n",
    "answer the question is still lacking.\n",
    "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
    "valuable information for answering the question.\n",
    "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
    "valuable information for answering the question.\n",
    "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
    "is enough to answer the question. This will allow you to summarize the gathered information and\n",
    "provide a final answer.\"\"\")\n",
    "\n",
    "chunk_read_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            chunk_read_system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Previous actions: {previous_actions}\n",
    "Notebook: {notebook}\n",
    "Chunk: {chunk}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chunk_read_chain = chunk_read_prompt | model.with_structured_output(ChunkOutput)\n",
    "\n",
    "def get_subsequent_chunk_id(chunk):\n",
    "    data = neo4j_graph.query(\"\"\"\n",
    "    MATCH (c:Chunk)-[:NEXT]->(next)\n",
    "    WHERE c.id = $id\n",
    "    RETURN next.id AS next\n",
    "    \"\"\")\n",
    "    return data\n",
    "\n",
    "def get_previous_chunk_id(chunk):\n",
    "    data = neo4j_graph.query(\"\"\"\n",
    "    MATCH (c:Chunk)<-[:NEXT]-(previous)\n",
    "    WHERE c.id = $id\n",
    "    RETURN previous.id AS previous\n",
    "    \"\"\")\n",
    "    return data\n",
    "\n",
    "def get_chunk(chunk_id: str) -> List[Dict[str, str]]:\n",
    "    data = neo4j_graph.query(\"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    WHERE c.id = $chunk_id\n",
    "    RETURN c.id AS chunk_id, c.text AS text\n",
    "    \"\"\", params={\"chunk_id\": chunk_id})\n",
    "    return data\n",
    "\n",
    "def chunk_check(state: OverallState) -> OverallState:\n",
    "    check_chunks_queue = state.get(\"check_chunks_queue\")\n",
    "    chunk_id = check_chunks_queue.pop()\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Step: read chunk({chunk_id})\")\n",
    "\n",
    "    chunks_text = get_chunk(chunk_id)\n",
    "    read_chunk_results = chunk_read_chain.invoke(\n",
    "        {\n",
    "            \"question\": state.get(\"question\"),\n",
    "            \"rational_plan\": state.get(\"rational_plan\"),\n",
    "            \"notebook\": state.get(\"notebook\"),\n",
    "            \"previous_actions\": state.get(\"previous_actions\"),\n",
    "            \"chunk\": chunks_text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    notebook = read_chunk_results.updated_notebook\n",
    "    print(\n",
    "        f\"Rational for next action after reading chunks: {read_chunk_results.rational_next_move}\"\n",
    "    )\n",
    "    chosen_action = parse_function(read_chunk_results.chosen_action)\n",
    "    print(f\"Chosen action: {chosen_action}\")\n",
    "    response = {\n",
    "        \"notebook\": notebook,\n",
    "        \"chosen_action\": chosen_action.get(\"function_name\"),\n",
    "        \"previous_actions\": [f\"read_chunks({chunk_id})\"],\n",
    "    }\n",
    "    if chosen_action.get(\"function_name\") == \"read_subsequent_chunk\":\n",
    "        subsequent_id = get_subsequent_chunk_id(chunk_id)\n",
    "        check_chunks_queue.append(subsequent_id)\n",
    "    elif chosen_action.get(\"function_name\") == \"read_previous_chunk\":\n",
    "        previous_id = get_previous_chunk_id(chunk_id)\n",
    "        check_chunks_queue.append(previous_id)\n",
    "    elif chosen_action.get(\"function_name\") == \"search_more\":\n",
    "        # Go over to next chunk\n",
    "        # Else explore neighbors\n",
    "        if not check_chunks_queue:\n",
    "            response[\"chosen_action\"] = \"search_neighbor\"\n",
    "            # Get neighbors/use vector similarity\n",
    "            print(f\"Neighbor rational: {read_chunk_results.rational_next_move}\")\n",
    "            neighbors = get_potential_nodes(\n",
    "                read_chunk_results.rational_next_move\n",
    "            )\n",
    "            response[\"neighbor_check_queue\"] = neighbors\n",
    "\n",
    "    response[\"check_chunks_queue\"] = check_chunks_queue\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ae4c1-e192-47f7-9d11-7b28f5e8b3fe",
   "metadata": {},
   "source": [
    "We start by popping a chunk ID from the queue and retrieving its text from the graph. Using the retrieved text and additional information from the overall state of the LangGraph system, we invoke the LLM chain. If the LLM decides it wants to read previous or subsequent chunks, we append their IDs to the queue. On the other hand, if the LLM chooses to search for more information, we have two options. If there are any other chunks to read in the queue, we move to reading them. Otherwise, we can use the vector search to get more relevant key elements and repeat the process by reading their atomic facts and so on.\n",
    "\n",
    "The paper is slightly dubious about the search_more function. On the one hand, it states that the search_more function can only read other chunks in the queue. On the other hand, in their example in the appendix, the function clearly explores the neighbors.\n",
    "\n",
    "To clarify, I emailed the authors, and they confirmed that the search_morefunction first tries to go through additional chunks in the queue. If none are present, it moves on to exploring the neighbors. Since how to explore the neighbors isn’t explicitly defined, we again use the vector similarity search to find potential nodes.\n",
    "\n",
    "## Neighbor selection\n",
    "When the LLM decides to explore the neighbors, we have helper functions to find potential key elements to explore. However, we don’t explore all of them. Instead, an LLM decides which of them is worth exploring, if any. Based on the provided potential neighbors, the LLM can decide which to explore. If none are worth exploring, it can decide to terminate the flow and move on to the answer reasoning step.\n",
    "\n",
    "The code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26942568-7ae9-426b-9237-3f53bd76e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_select_system_prompt = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
    "following elements:\n",
    "1. Text Chunks: Segments of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "Your current task is to assess all neighboring nodes of the current node, with the objective of determining whether to proceed to the next neighboring node. Given the question, rational\n",
    "plan, previous actions, notebook content, and the neighbors of the current node, you have the\n",
    "following Action Options:\n",
    "#####\n",
    "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
    "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
    "on one neighbor node at a time.\n",
    "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
    "information that could answer the question.\n",
    "#####\n",
    "Strategy:\n",
    "#####\n",
    "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
    "2. You can only choose one action. This means that you can choose to read only one neighbor\n",
    "node or choose to terminate.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin.\n",
    "\"\"\"\n",
    "\n",
    "class NeighborOutput(BaseModel):\n",
    "    rational_next_move: str = Field(description=\"\"\"Based on the given question, rational plan, previous actions, and\n",
    "notebook content, analyze how to choose the next action.\"\"\")\n",
    "    chosen_action: str = Field(description=\"\"\"You have the following Action Options:\n",
    "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
    "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
    "on one neighbor node at a time.\n",
    "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
    "information that could answer the question.\"\"\")\n",
    "\n",
    "neighbor_select_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            neighbor_select_system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Plan: {rational_plan}\n",
    "Previous actions: {previous_actions}\n",
    "Notebook: {notebook}\n",
    "Neighbor nodes: {nodes}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "neighbor_select_chain = neighbor_select_prompt | model.with_structured_output(NeighborOutput)\n",
    "\n",
    "def neighbor_select(state: OverallState) -> OverallState:\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Step: neighbor select\")\n",
    "    print(f\"Possible candidates: {state.get('neighbor_check_queue')}\")\n",
    "    neighbor_select_results = neighbor_select_chain.invoke(\n",
    "        {\n",
    "            \"question\": state.get(\"question\"),\n",
    "            \"rational_plan\": state.get(\"rational_plan\"),\n",
    "            \"notebook\": state.get(\"notebook\"),\n",
    "            \"nodes\": state.get(\"neighbor_check_queue\"),\n",
    "            \"previous_actions\": state.get(\"previous_actions\"),\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        f\"Rational for next action after selecting neighbor: {neighbor_select_results.rational_next_move}\"\n",
    "    )\n",
    "    chosen_action = parse_function(neighbor_select_results.chosen_action)\n",
    "    print(f\"Chosen action: {chosen_action}\")\n",
    "    # Empty neighbor select queue\n",
    "    response = {\n",
    "        \"chosen_action\": chosen_action.get(\"function_name\"),\n",
    "        \"neighbor_check_queue\": [],\n",
    "        \"previous_actions\": [\n",
    "            f\"neighbor_select({chosen_action.get('arguments', [''])[0] if chosen_action.get('arguments', ['']) else ''})\"\n",
    "        ],\n",
    "    }\n",
    "    if chosen_action.get(\"function_name\") == \"read_neighbor_node\":\n",
    "        response[\"check_atomic_facts_queue\"] = [\n",
    "            chosen_action.get(\"arguments\")[0]\n",
    "        ]\n",
    "    return response\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90b890-7611-4ef1-b55f-e2361e1a3e6e",
   "metadata": {},
   "source": [
    "Here, we execute the LLM chain and parse results. If the chosen action is to explore any neighbors, we add them to the check_atomic_facts_queue .\n",
    "\n",
    "Answer reasoning\n",
    "The last step in our flow is to ask the LLM to construct the final answer based on the collected information in the notebook.This node implementation is fairly straightforward as you can see by the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71098d92-8abc-4851-b491-bd36db1e8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_reasoning_system_prompt = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to answer questions based on information\n",
    "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
    "following elements:\n",
    "1. Text Chunks: Segments of the original text.\n",
    "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
    "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
    "facts derived from different text chunks.\n",
    "You have now explored multiple paths from various starting nodes on this graph, recording key information for each path in a notebook.\n",
    "Your task now is to analyze these memories and reason to answer the question.\n",
    "Strategy:\n",
    "#####\n",
    "1. You should first analyze each notebook content before providing a final answer.\n",
    "2. During the analysis, consider complementary information from other notes and employ a\n",
    "majority voting strategy to resolve any inconsistencies.\n",
    "3. When generating the final answer, ensure that you take into account all available information.\n",
    "#####\n",
    "Example:\n",
    "#####\n",
    "User:\n",
    "Question: Who had a longer tennis career, Danny or Alice?\n",
    "Notebook of different exploration paths:\n",
    "1. We only know that Danny’s tennis career started in 1972 and ended in 1990, but we don’t know\n",
    "the length of Alice’s career.\n",
    "2. ......\n",
    "Assistant:\n",
    "Analyze:\n",
    "The summary of search path 1 points out that Danny’s tennis career is 1990-1972=18 years.\n",
    "Although it does not indicate the length of Alice’s career, the summary of search path 2 finds this\n",
    "information, that is, the length of Alice’s tennis career is 15 years. Then we can get the final\n",
    "answer, that is, Danny’s tennis career is longer than Alice’s.\n",
    "Final answer:\n",
    "Danny’s tennis career is longer than Alice’s.\n",
    "#####\n",
    "Please strictly follow the above format. Let’s begin\n",
    "\"\"\"\n",
    "\n",
    "answer_reasoning_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            answer_reasoning_system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Question: {question}\n",
    "Notebook: {notebook}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class AnswerReasonOutput(BaseModel):\n",
    "    analyze: str = Field(description=\"\"\"You should first analyze each notebook content before providing a final answer.\n",
    "    During the analysis, consider complementary information from other notes and employ a\n",
    "majority voting strategy to resolve any inconsistencies.\"\"\")\n",
    "    final_answer: str = Field(description=\"\"\"When generating the final answer, ensure that you take into account all available information.\"\"\")\n",
    "\n",
    "answer_reasoning_chain = answer_reasoning_prompt | model.with_structured_output(AnswerReasonOutput)\n",
    "\n",
    "def answer_reasoning(state: OverallState) -> OutputState:\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Step: Answer Reasoning\")\n",
    "    final_answer = answer_reasoning_chain.invoke(\n",
    "        {\"question\": state.get(\"question\"), \"notebook\": state.get(\"notebook\")}\n",
    "    )\n",
    "    return {\n",
    "        \"answer\": final_answer.final_answer,\n",
    "        \"analysis\": final_answer.analyze,\n",
    "        \"previous_actions\": [\"answer_reasoning\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc1d6c-65a6-4036-b904-4b02d5ffe724",
   "metadata": {},
   "source": [
    "We simply input the original question and the notebook with the collected information to the chain and ask it to formulate the final answer and provide the explanation in the analysis part.\n",
    "\n",
    "## LangGraph flow definition\n",
    "The only thing left is to define the LangGraph flow and how it should traverse between the nodes. I am quite fond of the simple approach the LangChain team has chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b57214-a6db-4bf9-8616-4f1dd3ceac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_fact_condition(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"neighbor_select\", \"chunk_check\"]:\n",
    "    if state.get(\"chosen_action\") == \"stop_and_read_neighbor\":\n",
    "        return \"neighbor_select\"\n",
    "    elif state.get(\"chosen_action\") == \"read_chunk\":\n",
    "        return \"chunk_check\"\n",
    "\n",
    "def chunk_condition(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"answer_reasoning\", \"chunk_check\", \"neighbor_select\"]:\n",
    "    if state.get(\"chosen_action\") == \"termination\":\n",
    "        return \"answer_reasoning\"\n",
    "    elif state.get(\"chosen_action\") in [\"read_subsequent_chunk\", \"read_previous_chunk\", \"search_more\"]:\n",
    "        return \"chunk_check\"\n",
    "    elif state.get(\"chosen_action\") == \"search_neighbor\":\n",
    "        return \"neighbor_select\"\n",
    "\n",
    "def neighbor_condition(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"answer_reasoning\", \"atomic_fact_check\"]:\n",
    "    if state.get(\"chosen_action\") == \"termination\":\n",
    "        return \"answer_reasoning\"\n",
    "    elif state.get(\"chosen_action\") == \"read_neighbor_node\":\n",
    "        return \"atomic_fact_check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37d7ba-2847-42c8-9f68-20cc517334d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "langgraph = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "langgraph.add_node(rational_plan_node)\n",
    "langgraph.add_node(initial_node_selection)\n",
    "langgraph.add_node(atomic_fact_check)\n",
    "langgraph.add_node(chunk_check)\n",
    "langgraph.add_node(answer_reasoning)\n",
    "langgraph.add_node(neighbor_select)\n",
    "\n",
    "langgraph.add_edge(START, \"rational_plan_node\")\n",
    "langgraph.add_edge(\"rational_plan_node\", \"initial_node_selection\")\n",
    "langgraph.add_edge(\"initial_node_selection\", \"atomic_fact_check\")\n",
    "langgraph.add_conditional_edges(\n",
    "    \"atomic_fact_check\",\n",
    "    atomic_fact_condition,\n",
    ")\n",
    "langgraph.add_conditional_edges(\n",
    "    \"chunk_check\",\n",
    "    chunk_condition,\n",
    ")\n",
    "langgraph.add_conditional_edges(\n",
    "    \"neighbor_select\",\n",
    "    neighbor_condition,\n",
    ")\n",
    "langgraph.add_edge(\"answer_reasoning\", END)\n",
    "\n",
    "langgraph = langgraph.compile()\n",
    "\n",
    "# View\n",
    "display(Image(langgraph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c197e-4892-4398-98ed-4a9918a5258a",
   "metadata": {},
   "source": [
    "We begin by defining the state graph object, where we can define the information passed along in the LangGraph. Each node is simply added with the add_node method. Normal edges, where one step always follows the other, can be added with a add_edge method. On the other hand, if the traversals is dependent on previous actions, we can use the add_conditional_edge and pass in the function that selects the next node.\n",
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c45f8-1f41-4e0a-a921-9b3f51b7adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "langgraph.invoke({\"question\":\"What is Patient name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41c1ab-7973-4e20-888a-2be5c94475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "langgraph.invoke({\"question\":\"Is there any anomaly in her report?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21900334-d1dc-4761-a576-61c4569f920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langgraph.invoke({\"question\":\"Any specific problem identified in the report?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259d73f-1f43-483e-86e6-ba28fbc0d229",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
